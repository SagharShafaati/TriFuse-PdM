{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a91d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Predictive Maintenance Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, classification_report, confusion_matrix,\n",
    "    precision_recall_curve, auc, matthews_corrcoef, roc_curve\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import shap\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ========== Setup ==========\n",
    "output_dir = \"result\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ========== Load and Preprocess Dataset ==========\n",
    "file_path = 'ai4i2020.csv'  # <--- UPDATE this path to your dataset location\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.drop(columns=['UDI', 'Product ID', 'Type'], inplace=True, errors='ignore')\n",
    "df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True).str.strip()\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "high_corr_features = ['HDF', 'OSF', 'PWF', 'TWF']\n",
    "df_cleaned = df_imputed.drop(columns=[col for col in high_corr_features if col in df_imputed.columns])\n",
    "\n",
    "target = 'Machine_failure'\n",
    "features = [col for col in df_cleaned.columns if col != target]\n",
    "\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned[target]\n",
    "\n",
    "# ========== Hybrid Resampling (SMOTE + Tomek) ==========\n",
    "print(\"Applying SMOTE + Tomek Links...\")\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X, y = smote_tomek.fit_resample(X, y)\n",
    "print(\"New class distribution:\\n\", pd.Series(y).value_counts())\n",
    "\n",
    "# ========== Train/Test Split ==========\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ========== Cost-Sensitive Models ==========\n",
    "param_grid_rf = {'n_estimators': [100], 'max_depth': [10]}\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "                              param_grid_rf, cv=3)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "param_grid_xgb = {'n_estimators': [100], 'max_depth': [5], 'learning_rate': [0.1]}\n",
    "grid_search_xgb = GridSearchCV(XGBClassifier(scale_pos_weight=scale_pos_weight, use_label_encoder=False,\n",
    "                                             eval_metric='logloss'), param_grid_xgb, cv=3)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "log_model = LogisticRegressionCV(class_weight='balanced', cv=3, penalty='l2', solver='liblinear')\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# ========== Deep Learning Model (MLP) ==========\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_dl.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "mlp.fit(X_train_dl, y_train_dl, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stop])\n",
    "y_prob_dl = mlp.predict(X_test_dl).ravel()\n",
    "y_pred_dl = (y_prob_dl > 0.5).astype(int)\n",
    "\n",
    "# ========== Evaluation Function ==========\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_advanced(y_true, y_prob, model_name):\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}, MCC: {mcc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# ========== Evaluate Models ==========\n",
    "models = {\n",
    "    'Random Forest': (best_rf, X_test),\n",
    "    'XGBoost': (best_xgb, X_test),\n",
    "    'Logistic Regression': (log_model, X_test),\n",
    "    'MLP': (mlp, X_test_dl)\n",
    "}\n",
    "\n",
    "probs = {}\n",
    "for name, (model, X_eval) in models.items():\n",
    "    if name == 'MLP':\n",
    "        y_prob = model.predict(X_eval).ravel()\n",
    "    else:\n",
    "        y_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    evaluate_advanced(y_test, y_prob, name)\n",
    "    probs[name] = y_prob\n",
    "\n",
    "# ========== Calibration Curve ==========\n",
    "def plot_calibration_curve(y_true, y_prob, model_name):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    plt.figure()\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f'{model_name}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Observed Frequency')\n",
    "    plt.title(f'Calibration Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{output_dir}/calibration_{model_name}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "for name, y_prob in probs.items():\n",
    "    plot_calibration_curve(y_test, y_prob, name)\n",
    "\n",
    "# ========== SHAP Explainability (Random Forest) ==========\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values[1], X_train)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/shap_summary_rf.png\", dpi=300)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
